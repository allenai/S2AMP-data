{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b07e8ad9-66a0-4eb9-80b8-f10e0d294828",
   "metadata": {},
   "source": [
    "## Second stage feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233ac18f-6adb-4d71-b06f-745447add6b7",
   "metadata": {},
   "source": [
    "This program will take the features from the first stage and the first stage model.\n",
    "It will build a graph using the predictions from first stage and then extract graph features and \n",
    "write them to csv files.\n",
    "\n",
    "These features can be used later to get more accurate predictions using the second stage model\n",
    "\n",
    "To run this make sure you have enough RAM. The program creates a graph of 200 million nodes which will require around 120G of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acfa9dfd-6f20-4d92-adfb-c47645ed2828",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_tool.all import *\n",
    "from second_stage import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "082d774e-1eb5-4a54-8a4e-cb893d633f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the first stage features : 52G\n",
    "\n",
    "#!aws s3 cp --no-sign-request s3://ai2-s2-research-public/s2amp/inferred/first_stage_features/ data/inferred/first_stage_features/\n",
    "#!aws s3 cp --no-sign-request s3://ai2-s2-research-public/s2amp/gold/lgb_first.stage.model.pkl data/gold/lgb_first.stage.model.pkl\n",
    "\n",
    "inferred_first_stage_features_dir = 'data/inferred/first_stage_features/'\n",
    "first_stage_model = 'data/gold/lgb_first.stage.model.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5647fb47-1648-4bb0-9ebd-135d58f5f7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make directory second_stage_features\n",
    "os.makedirs(\"second_stage_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c28b511c-1fe3-4097-8542-5d8996016491",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:reading first stage features . . \n",
      "100%|██████████| 200/200 [09:09<00:00,  2.75s/it]\n"
     ]
    }
   ],
   "source": [
    "# reading first stage pairwise features\n",
    "df_features = read_features(inferred_first_stage_features_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b05a7801-ea10-4403-a56e-048df0f2382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting predictions for first stage\n",
    "df_pred = get_prediction_feature_dataframe(df_features, first_stage_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f18bb31-58c6-4c5f-8ca6-b8ed64d511d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building weighted graph\n",
    "get_directed_mentor_mentee_graph(df_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "549119f7-bc20-4be8-a731-9a903725c9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now the graph has been built we can get rid of the huge df\n",
    "del df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782c3f12-6e3e-4e9e-a79b-4828696dec47",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# getting graph features for each pair in the first stage feature file\n",
    "# this will output around 400 csv files with features\n",
    "\n",
    "count = 0\n",
    "for filename in tqdm(glob.glob(inferred_first_stage_features_dir + \"/*.csv\")):\n",
    "    df = pd.read_csv(filename)\n",
    "    # splitting huge file into smaller chunks\n",
    "    df_splits = np.array_split(df, 5)\n",
    "\n",
    "    # processing each chunk using the multiprocessing pool\n",
    "    for df_split in df_splits:\n",
    "        df_graph_features = parallelize_dataframe(\n",
    "            df_split, get_graph_features, n_cores=10,\n",
    "        )\n",
    "        df_graph_features.to_csv(\n",
    "            \"second_stage_features/features.\" + str(count) + \".csv\", index=False,\n",
    "        )\n",
    "        count += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}